{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e14efbb",
   "metadata": {},
   "source": [
    "# リカレントニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d5608b",
   "metadata": {},
   "source": [
    "最初の時間ステップt=0では、隠れユニットがそれぞれ0または小さな乱数で初期化される。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398107a1",
   "metadata": {},
   "source": [
    "## 長期的な相互作用の学習\n",
    "BPTT(Backpropagation Through Time)により、勾配消失問題や勾配発散問題が発生する。解決策として、\n",
    "* 勾配刈り込み（勾配の閾値を設定）\n",
    "* T-BPTT(Truncated Backpropagation Through Time)（逆伝搬できる時間ステップの数を制限する）\n",
    "* 長短期記憶(Long Short-Term Memory:LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc924c",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e2e9dc",
   "metadata": {},
   "source": [
    "LSTM（Long Short-Term Memory）の数式の根拠は、「**情報を長期的に保持し、必要なときに取り出す**」という目的のもと、**勾配消失問題（vanishing gradient）を回避する構造**を持つことにあります。以下に、LSTMの数式とそれぞれの根拠を解説します。\n",
    "\n",
    "---\n",
    "\n",
    "### LSTMの基本構造と数式\n",
    "\n",
    "時間ステップ $t$ におけるLSTMの更新式は次のとおりです（入力 $\\mathbf{x}_t$、前の隠れ状態 $\\mathbf{h}_{t-1}$、セル状態 $\\mathbf{c}_{t-1}$）：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{i}_t &= \\sigma(\\mathbf{W}_i \\mathbf{x}_t + \\mathbf{U}_i \\mathbf{h}_{t-1} + \\mathbf{b}_i) \\quad &\\text{（入力ゲート）}\\\\\n",
    "\\mathbf{f}_t &= \\sigma(\\mathbf{W}_f \\mathbf{x}_t + \\mathbf{U}_f \\mathbf{h}_{t-1} + \\mathbf{b}_f) \\quad &\\text{（忘却ゲート）}\\\\\n",
    "\\mathbf{o}_t &= \\sigma(\\mathbf{W}_o \\mathbf{x}_t + \\mathbf{U}_o \\mathbf{h}_{t-1} + \\mathbf{b}_o) \\quad &\\text{（出力ゲート）}\\\\\n",
    "\\tilde{\\mathbf{c}}_t &= \\tanh(\\mathbf{W}_c \\mathbf{x}_t + \\mathbf{U}_c \\mathbf{h}_{t-1} + \\mathbf{b}_c) \\quad &\\text{（候補セル状態）}\\\\\n",
    "\\mathbf{c}_t &= \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{c}}_t \\quad &\\text{（セル状態の更新）}\\\\\n",
    "\\mathbf{h}_t &= \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t) \\quad &\\text{（出力）}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 数式の根拠と設計思想\n",
    "\n",
    "| 数式                           | 意図・根拠                                                 |\n",
    "| ---------------------------- | ----------------------------------------------------- |\n",
    "| $\\mathbf{i}_t$（入力ゲート）        | 新しい情報をどれだけセルに加えるか制御する。重要な入力だけ通す。                      |\n",
    "| $\\mathbf{f}_t$（忘却ゲート）        | セルに保持された情報をどれだけ忘れるかを決める。長期依存を制御。<br>→ **勾配消失の回避**にも貢献 |\n",
    "| $\\tilde{\\mathbf{c}}_t$（候補セル） | 新しく加えたい候補情報。tanhを使って情報のスケーリング（\\[-1,1]に制限）             |\n",
    "| $\\mathbf{c}_t$（セル状態）         | **LSTMの核心**。時間を超えて情報を伝える経路。加算により勾配消失を防ぐ               |\n",
    "| $\\mathbf{o}_t$（出力ゲート）        | セルからどれだけ外に出力するか決める。必要な情報だけ抽出                          |\n",
    "| $\\mathbf{h}_t$（出力）           | 実際に次のRNNステップや出力に使われる値                                 |\n",
    "\n",
    "---\n",
    "\n",
    "### 根拠まとめ\n",
    "\n",
    "1. **勾配消失を避ける設計**\n",
    "\n",
    "   * セル状態 $\\mathbf{c}_t$ は加算で更新されるため、時間方向に勾配が直接流れやすい（BPTTでの安定性）。\n",
    "   * これは従来のRNN（$\\mathbf{h}_t = \\tanh(Wx_t + Uh_{t-1})$）では難しかった。\n",
    "\n",
    "2. **学習可能なメモリ制御**\n",
    "\n",
    "   * 忘却・入力・出力ゲートがそれぞれ独立に学習されるため、文脈依存の動的な制御が可能。\n",
    "\n",
    "3. **ゲート構造による柔軟な情報選択**\n",
    "\n",
    "   * 重要な情報のみを通し、不要な情報を遮断・忘却できる。\n",
    "\n",
    "---\n",
    "\n",
    "### 関連論文と歴史的背景\n",
    "\n",
    "* 初出論文：**Hochreiter and Schmidhuber, 1997**, [\"Long Short-Term Memory\"](https://www.bioinf.jku.at/publications/older/2604.pdf)\n",
    "* 背景：従来のRNNでは長期依存がうまく学習できなかった（勾配消失・爆発）。それに対する解決策として提案。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744eb37c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4832e4d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
