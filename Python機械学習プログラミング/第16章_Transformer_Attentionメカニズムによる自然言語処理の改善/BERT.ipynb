{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41979229",
   "metadata": {},
   "source": [
    "## PyTorchでのBERTモデルのファインチューニング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5a4ec8",
   "metadata": {},
   "source": [
    "BERTはモデルの知名度が非常に高く、それでいてGPUが1つだけでもファインチューニングを実行できる扱いやすいサイズである点でバランスが良い。  \n",
    "DistilBERT：事前訓練済みのBERTモデルをベースモデルとし、このモデルを抽出することで作成された軽量なTransformerモデル。元のモデルには、110,000,000個あまりのパラメータが含まれているが、40%ほど少なくなっている。また、60%高速である一方、GLUEでBERTの95%の性能を維持している。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca006af",
   "metadata": {},
   "source": [
    "### IMDb映画レビューデータセット"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76097bcb",
   "metadata": {},
   "source": [
    "```\n",
    "export PYENV_ROOT=\"$HOME/.pyenv\"\n",
    "export PATH=\"$PYENV_ROOT/bin:$PATH\"\n",
    "eval \"$(pyenv init -)\"\n",
    "eval \"$(pyenv virtualenv-init -)\"\n",
    "\n",
    "source ~/.zshrc\n",
    "\n",
    "pyenv activate torchenv310\n",
    "\n",
    "pip freeze > requirements_pyenv.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb58b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e777f664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "NUM_EPOCHS = 3\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be46f466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I normally do not watch local mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Believe it or not, this was at one time the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After some internet surfing, I found the \"Home...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the most unheralded great works of anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was the Sixties, and anyone with long hair ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  My family and I normally do not watch local mo...          1\n",
       "1  Believe it or not, this was at one time the wo...          0\n",
       "2  After some internet surfing, I found the \"Home...          0\n",
       "3  One of the most unheralded great works of anim...          1\n",
       "4  It was the Sixties, and anyone with long hair ...          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='../第8章_機械学習の適用_感情分析'\n",
    "df = pd.read_csv(f'{path}/movie_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8159726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df.iloc[:35000]['review'].values\n",
    "train_labels = df.iloc[:35000]['sentiment'].values\n",
    "valid_texts = df.iloc[35000:40000]['review'].values\n",
    "valid_labels = df.iloc[35000:40000]['sentiment'].values\n",
    "test_texts = df.iloc[40000:]['review'].values\n",
    "test_labels = df.iloc[40000:]['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06b0871",
   "metadata": {},
   "source": [
    "### データセットのトークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ec7c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokennizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokennizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "valid_encodings = tokennizer(valid_texts.tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokennizer(test_texts.tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cad23f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f47655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "valid_dataset = IMDbDataset(valid_encodings, valid_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863b6ab6",
   "metadata": {},
   "source": [
    "### モデルの読み込みとファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5d6e4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased').to(DEVICE)\n",
    "model.train()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e534954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    with torch.no_grad():\n",
    "        correct_pred, num_examples = 0, 0\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            # データの前処理\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            predicted_labels = torch.argmax(logits, 1)\n",
    "            num_examples += labels.size(0)\n",
    "            correct_pred += (predicted_labels == labels).sum()\n",
    "    return correct_pred.float() / num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1f5db2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0001/0003, Batch 0000/2188, Loss: 0.6993\n",
      "Epoch 0001/0003, Batch 0250/2188, Loss: 0.2853\n",
      "Epoch 0001/0003, Batch 0500/2188, Loss: 0.2672\n",
      "Epoch 0001/0003, Batch 0750/2188, Loss: 0.6540\n",
      "Epoch 0001/0003, Batch 1000/2188, Loss: 0.1824\n",
      "Epoch 0001/0003, Batch 1250/2188, Loss: 0.1600\n",
      "Epoch 0001/0003, Batch 1500/2188, Loss: 0.1812\n",
      "Epoch 0001/0003, Batch 1750/2188, Loss: 0.1060\n",
      "Epoch 0001/0003, Batch 2000/2188, Loss: 0.0925\n",
      "Training accuracy: 96.84%\n",
      "Valid accuracy: 93.04%\n",
      "Time elapsed: 54.06 min\n",
      "Epoch 0002/0003, Batch 0000/2188, Loss: 0.3438\n",
      "Epoch 0002/0003, Batch 0250/2188, Loss: 0.1447\n",
      "Epoch 0002/0003, Batch 0500/2188, Loss: 0.0519\n",
      "Epoch 0002/0003, Batch 0750/2188, Loss: 0.2491\n",
      "Epoch 0002/0003, Batch 1000/2188, Loss: 0.0637\n",
      "Epoch 0002/0003, Batch 1250/2188, Loss: 0.0365\n",
      "Epoch 0002/0003, Batch 1500/2188, Loss: 0.0506\n",
      "Epoch 0002/0003, Batch 1750/2188, Loss: 0.1599\n",
      "Epoch 0002/0003, Batch 2000/2188, Loss: 0.0301\n",
      "Training accuracy: 98.61%\n",
      "Valid accuracy: 93.34%\n",
      "Time elapsed: 111.26 min\n",
      "Epoch 0003/0003, Batch 0000/2188, Loss: 0.0854\n",
      "Epoch 0003/0003, Batch 0250/2188, Loss: 0.0115\n",
      "Epoch 0003/0003, Batch 0500/2188, Loss: 0.0119\n",
      "Epoch 0003/0003, Batch 0750/2188, Loss: 0.0281\n",
      "Epoch 0003/0003, Batch 1000/2188, Loss: 0.0314\n",
      "Epoch 0003/0003, Batch 1250/2188, Loss: 0.0400\n",
      "Epoch 0003/0003, Batch 1500/2188, Loss: 0.0972\n",
      "Epoch 0003/0003, Batch 1750/2188, Loss: 0.0435\n",
      "Epoch 0003/0003, Batch 2000/2188, Loss: 0.0153\n",
      "Training accuracy: 98.99%\n",
      "Valid accuracy: 92.78%\n",
      "Time elapsed: 167.68 min\n",
      "Total training time: 167.68 min\n",
      "Test accuracy: 92.17%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # データの前処理\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        # モデルの出力\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "\n",
    "        # 勾配の計算とパラメータの更新\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if batch_idx % 250 == 0:\n",
    "            print(f'Epoch {epoch + 1:04d}/{NUM_EPOCHS:04d}, Batch {batch_idx:04d}/{len(train_loader):04d}, Loss: {loss:.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'Training accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nValid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "\n",
    "print(f'Total training time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69a2651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'BERT.pt'\n",
    "torch.save(model, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac9057f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
