{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e18d0d",
   "metadata": {},
   "source": [
    "# 勾配ブースティング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d33cf06",
   "metadata": {},
   "source": [
    "各分類木（弱学習器）の予測誤差（＝損失の勾配）を、次の分類木で学習していく。\n",
    "葉に相当する分類木の最大の深さはだいたい３から６である。 （ AdaBoostは深さ1の木が多い。）\n",
    "AdaBoostとは対照的に、勾配ブースティングは予測誤差を訓練データの重みの割り当てに使うのではなく、次の決定木を訓練するための目的変数の設定に直接使う。さらに、決定木ごとに重みの項を使うのではなく、全体で1つの学習率（全て同じ値）を使う。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02d22c6",
   "metadata": {},
   "source": [
    "勾配ブースティングはsklearn.ensemble.GradientBoostingClassifierとして実装されているが、勾配ブースティングが逐次的なプロセスであり、その訓練に時間がかかる可能性がある。XGBoostの方が人気。他にもLightGBMやCatBoostなぢ、いくつかの勾配ブースティング実装が人気を集めている。  \n",
    "scikit-learnはLightGBMにヒントを得て、GradientBoostingClassifierよりも高性能なHistGradientBoostingClassifierを実装している。  \n",
    "序盤の木と終盤の木の重みが同じなのが違和感。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58eddac",
   "metadata": {},
   "source": [
    "[参考サイト](https://zenn.dev/dalab/articles/9c843f0ec8aabf)  \n",
    "ただ、2本目以降の決定木の目的変数は$y-\\Sigma \\hat{y}$とすべきだと思う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce86c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'\n",
    "                      'machine-learning-databases/wine/wine.data',\n",
    "                      header=None)\n",
    "\n",
    "df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n",
    "                   'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',\n",
    "                   'Proline']\n",
    "\n",
    "# drop 1 class\n",
    "df_wine = df_wine[df_wine['Class label'] != 1]\n",
    "\n",
    "y = df_wine['Class label'].values\n",
    "X = df_wine[['Alcohol', 'Hue']].values\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "            train_test_split(X, y, \n",
    "                             test_size=0.40, \n",
    "                             random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfbb17e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nagairyousuke/機械学習Python/StudyML/StudyML/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [19:06:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost train/test accuracies: 0.986/0.896\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = xgb.XGBClassifier(n_estimators=1000, learning_rate=0.01,\n",
    "                          max_depth=4,random_state=1, use_label_encoder=False)\n",
    "gbm = model.fit(X_train, y_train)\n",
    "y_train_pred = gbm.predict(X_train)\n",
    "y_test_pred = gbm.predict(X_test)\n",
    "gbm_train = accuracy_score(y_train, y_train_pred)\n",
    "gbm_test = accuracy_score(y_test, y_test_pred)\n",
    "print(f'XGBoost train/test accuracies: {gbm_train:.3f}/{gbm_test:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252c3a6",
   "metadata": {},
   "source": [
    "## アルゴリズム\n",
    "もちろんです！\n",
    "ここでは、\\*\\*勾配ブースティング（Gradient Boosting）\\*\\*のアルゴリズムを「ステップごとに」「分類と回帰の両方に通じる一般形」でわかりやすく解説します。\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 勾配ブースティングの概要\n",
    "\n",
    "**目的：**\n",
    "複数の「弱学習器（通常は決定木）」を**逐次的に組み合わせて**、最終的に高精度な予測モデルを構築する。\n",
    "\n",
    "**コアアイデア：**\n",
    "「前回の予測の誤り（損失関数の勾配）を説明するように、次の学習器を学習させる」\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 アルゴリズム（一般形）\n",
    "\n",
    "与えられた：\n",
    "\n",
    "* 入力データ $\\{(x_1, y_1), ..., (x_N, y_N)\\}$\n",
    "* 損失関数 $L(y, F(x))$（例：平方誤差、ロジスティック損失など）\n",
    "* 初期モデル $F_0(x)$\n",
    "* 学習率（learning rate） $\\eta \\in (0,1]$\n",
    "* イテレーション数 $M$\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 ステップごとの流れ：\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1. **初期化**\n",
    "\n",
    "$$\n",
    "F_0(x) = \\arg\\min_{\\gamma} \\sum_{i=1}^{N} L(y_i, \\gamma)\n",
    "$$\n",
    "\n",
    "* 例：回帰（平方誤差）の場合、$F_0(x) = \\text{平均}(y_i)$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2. **Boosting を繰り返す（m = 1 to M）**\n",
    "\n",
    "#### Step 2-1. 勾配（または残差）を計算：\n",
    "\n",
    "各データ点 $i$ について：\n",
    "\n",
    "$$\n",
    "r_{im} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x_i) = F_{m-1}(x_i)}\n",
    "$$\n",
    "\n",
    "* これは「**今のモデルでどのくらいズレてるか**」の指標（勾配 or 残差）\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2-2. 残差 $r_{im}$ をターゲットとして、回帰木 $h_m(x)$ を学習：\n",
    "現在のモデルの損失関数に対する負の勾配（residual）を近似するように学習器 を訓練する。\n",
    "**損失関数を最も減らす方向（＝負の勾配方向）を目的変数**として、次の決定木を学習。\n",
    "\n",
    "$$\n",
    "x_i \\mapsto r_{im}\n",
    "\\quad \\text{で学習し、} \\quad h_m(x) = \\sum_{j=1}^{J_m} \\gamma_{jm} \\cdot \\mathbf{1}_{x \\in R_{jm}}\n",
    "$$\n",
    "\n",
    "* データを $J_m$ 個のリージョン（葉）$R_{jm}$ に分割\n",
    "* 各葉での出力値 $\\gamma_{jm}$ を求める\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2-3. 各リージョン $R_{jm}$ ごとに最適な補正量 $\\gamma_{jm}$ を決定：\n",
    "\n",
    "$$\n",
    "\\gamma_{jm} = \\arg\\min_{\\gamma} \\sum_{x_i \\in R_{jm}} L(y_i, F_{m-1}(x_i) + \\gamma)\n",
    "$$\n",
    "\n",
    "$\\gamma_{jm}$の最適化だけで、勾配が表には出てこないが、しっかり勾配と2階導関数と同義となっている。  \n",
    "単なる勾配を使って更新するより厳密。\n",
    "* この補正量は「このリージョンに属する点を、どれくらい補正すれば損失が最小になるか」\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2-4. モデルを更新：\n",
    "\n",
    "$$\n",
    "F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)\n",
    "$$\n",
    "\n",
    "* $\\eta$ は学習率。値が小さいほど学習は遅いが安定。\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3. 最終モデル出力：\n",
    "\n",
    "$$\n",
    "F_M(x) = F_0(x) + \\eta \\sum_{m=1}^{M} h_m(x)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 具体例（回帰の場合）\n",
    "\n",
    "* 損失関数 $L(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2$\n",
    "* 勾配は $r_i = y_i - F_{m-1}(x_i)$（＝残差）\n",
    "* 木はこの残差を予測するように学習され、補正量 $\\gamma_{jm}$ は単純な平均\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ ポイントまとめ\n",
    "\n",
    "| 項目         | 内容                        |\n",
    "| ---------- | ------------------------- |\n",
    "| モデル構築方法    | 決定木を逐次的に追加                |\n",
    "| 学習目標       | 前回までの誤差（勾配）を修正            |\n",
    "| 学習率 $\\eta$ | 過学習を防ぎながら更新するための係数        |\n",
    "| 出力値（葉の値）   | 損失を最小にする補正量 $\\gamma_{jm}$ |\n",
    "\n",
    "### 勘違い\n",
    "木が連なるわけではない。各決定木は独立。  \n",
    "誤差が小さくなるようにどんどん木を作っていく。どこの分岐になるかによって加算される値は変わる。  \n",
    "序盤の木と終盤の木の重みが同じなのが違和感。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a035e90",
   "metadata": {},
   "source": [
    "### ✅ なぜすべて同じ重みで足すのか？\n",
    "\n",
    "これは勾配ブースティングの基本的な設計思想に基づいています：\n",
    "\n",
    "* 各ステップでは「損失関数の勾配方向」に沿って少しだけ修正する（＝微調整する）\n",
    "* その「ステップ幅」を一定にすることで、**過学習を防ぎつつ安定した収束**を目指す\n",
    "* この「ステップ幅」に相当するのが **学習率（learning rate, η）**\n",
    "\n",
    "---\n",
    "\n",
    "### ❗ でも、1番目とK番目の木を等しく扱うのは問題？\n",
    "\n",
    "確かに、現実には：\n",
    "\n",
    "* **初期の木は大きな誤差を修正**し、\n",
    "* **後半の木は微調整をしている**だけ\n",
    "\n",
    "なので、同じ重みで扱うのが最適とは限りません。\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠 そこで登場するのが「重み付け」や「正則化」\n",
    "\n",
    "以下のような改良版がよく使われます：\n",
    "\n",
    "#### ① **Shrinkage（縮小）**\n",
    "\n",
    "* 各木の出力に学習率 $\\eta$ を掛けて「弱く」する\n",
    "* すべて同じ η だが、小さな η にして**たくさんの木で調整する**\n",
    "\n",
    "#### ② **Weighted Boosting**\n",
    "\n",
    "* 各ステップで木に異なる重み（たとえばデータ分割に応じた γ）を導入する方法もあり\n",
    "* 例：XGBoostでは、葉の出力に正則化項を含めて最適な出力値を計算している\n",
    "\n",
    "#### ③ **早期終了（early stopping）**\n",
    "\n",
    "* 木の数を増やしすぎると過学習になるので、途中で打ち切る（Kが大きすぎないようにする）\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 実装例：XGBoostやLightGBM\n",
    "\n",
    "* XGBoostでは、各木の出力（葉ごとの値）を最小化された目的関数に基づいて決定\n",
    "* このとき、正則化項を入れて「重要でない木の影響を小さくする」\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 まとめ\n",
    "\n",
    "| 観点               | 説明                                               |\n",
    "| ---------------- | ------------------------------------------------ |\n",
    "| 全ての木に同じ重みを与えるのは？ | ✔️ 基本形としてはOK                                     |\n",
    "| 現実には？            | ❗ 最適でない場合も多い                                     |\n",
    "| 改良策              | 学習率（η）の調整、正則化、early stopping、XGBoostのような高度な重み最適化 |\n",
    "\n",
    "---\n",
    "\n",
    "必要なら、XGBoostの目的関数の導出や、葉ごとの出力の最適化についても数式込みで説明できます！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8d2a8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StudyML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
